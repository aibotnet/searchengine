<html>
<head>
<title>Search Engine</title>
</head>

<body topmargin="0" leftmargin="0" rightmargin="0" bottommargin="0" marginheight="0" marginwidth="0" bgcolor="#FFFFFF">

<div align="center">
  <center>
  <table border="0" width="90%" cellspacing="0" cellpadding="0">
    <tr>
      <td width="50%"><img border="0" src="img/topleft.jpg" width="372" height="106"></td>
    </center>
    <td width="50%">
      <p align="right"><img border="0" src="img/logo.jpg" width="217" height="64"></td>
  </tr>
  </table>
</div>
<div align="center">
  <center>
  <table border="0" width="90%" cellspacing="0" cellpadding="0" background="img/bluebar.jpg">
    <tr>
      <td width="100%"><font size="1">&nbsp;</font></td>
    </tr>
  </table>
  </center>
</div>
<div align="center">
  <center>
  <table border="0" width="90%" cellspacing="0" cellpadding="0" style="text-align: justify;text-justify: inter-word;">
    <tr>
      <td width="99%" valign="top"><br>
        <p>
        <font size="5" style="color:darkblue;">Search Engine<br></font>
        <font face="Arial" size="4">
            A search engine is a software program or script available through the Internet that searches documents and files for keywords and returns the results of any files
            containing those keywords. The first search engine ever developed is considered Archie, which was used to search for FTP files and the first text-based search
            engine is considered Veronica.Because large search engines contain millions and sometimes billions of pages, many search engines not only just search the pages
            but also display the results depending upon their importance. This importance is commonly determined by using various algorithms.<br><br>
        </font>
        <font size="5" style="color:darkblue;">Steps Involved In Search Engine</font><br>
          <div style="margin-left: 40px;">
          <font face="Arial" size="4">
          1. Crawling<br>
          2. Tokenizing<br> 
          3. Parsing<br>
          4. Normalization[capitalization,synonymous database, Abbreviation]<br> 
          5. Stemming [play,playing,played => play]<br>
          6. Indexing<br>
          7. Ranking Document<br><br>
          </font>
          </div>
          <font size="5" style="color:darkblue;">Crawling</font><br>
          <font face="Arial" size="3">
          A Web crawler starts with a list of URLs to visit, called the seeds. As the crawler visits these URLs, it identifies all the hyperlinks in the page
          and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies.
          If the crawler is performing archiving of websites it copies and saves the information as it goes. Such archives are usually stored such that they
          can be viewed, read and navigated as they were on the live web, but are preserved as â€˜snapshots'. The number of possible URLs crawled being generated
          by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content.<br>
          </div>
          <font size="4" style="color:darkblue;">
          Crawling policy[The behavior of a Web crawler is the outcome of a combination of policies]<br>
          </font>
          <font face="Arial" size="3">
            <div style="margin-left:40px">
              a) Selection policy that states which pages to download.<br>
              b) Re-visit policy that states when to check for changes to the pages.<br>
              c) Politeness policy that states how to avoid overloading Web sites.<br>
              d) Parallelization policy that states how to coordinate distributed web crawlers.<br>
            </div>
          </font>
          <font size="4" style="color:darkblue;">
          Selection policy<br>
          </font>
          <font face="Arial" size="3">
          Given the current size of the Web, even large search engines cover only a portion of the publicly available part.
          A 2009 study showed that large-scale search engines index no more than 40-70% of the indexable Web; a previous 
          study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999.
          As a crawler always downloads just a fraction of the Web pages, it is highly desirable that the downloaded 
          fraction contains the most relevant pages and not just a random sample of the Web.
          This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its 
          intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of 
          vertical search engines restricted to a single top-level domain, or search engines restricted to a fixed Web site). 
          Designing a good selection policy has an added difficulty: it must work with partial information, as the complete 
          set of Web pages is not known during crawling<br>
          </font>
          <font size="4" style="color:darkblue;">
          URL Normalization<br>
          </font>
          <font face="Arial" size="3">
          Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once.
          The term URL normalization, also called URL canonicalization, refers to the process of modifying and standardizing 
          a URL in a consistent manner. There are several types of normalization that may be performed including conversion 
          of URLs to lowercase, removal of "." and ".." segments, and adding trailing slashes to the non-empty path 
          component.<br>
          </font>
          <font size="4" style="color:darkblue;">
          Path-Ascending Crawling<br>
          </font>
          <font face="Arial" size="3">
          Some crawlers intend to download as many resources as possible from a particular web site. So path-ascending crawler
          was introduced that would ascend to every path in each URL that it intends to crawl. For example, when given a seed
          URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. 
          Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for 
          which no inbound link would have been found in regular crawling.<br>
          </font>
          <font size="4" style="color:darkblue;">
          URL Normalization<br>
          </font>
          <font face="Arial" size="3">
          Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once.
          The term URL normalization, also called URL canonicalization, refers to the process of modifying and standardizing 
          a URL in a consistent manner. There are several types of normalization that may be performed including conversion 
          of URLs to lowercase, removal of "." and ".." segments, and adding trailing slashes to the non-empty path 
          component.<br>
          </font>
          <font size="4" style="color:darkblue;">
          Focused crawling<br>
          </font>
          <font face="Arial" size="3">
          The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given 
          query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or 
          topical crawlers.<br>
          </font>
          <font size="4" style="color:darkblue;">
          Academic-Focused Crawler<br>
          </font>
          <font face="Arial" size="3">
          An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents. 
          Most academic papers are published in PDF formats, such kind of crawler is particularly interested in 
          crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open 
          source crawlers, such as Heritrix, must be customized to filter out other MIME types, or a middleware is used 
          to extract these documents out and import them to the focused crawl database and repository.<br><br>
          </font>
          <font size="4" style="color:darkblue;">
          Re-Visit Policy<br>
          </font>
          <font face="Arial" size="3">
          The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web 
          crawler has finished its crawl, many events could have happened, including creations, updates and deletions.<br>
          From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an 
          outdated copy of a resource. The most-used cost functions are freshness and age.<br>

          Freshness: This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a 
          page p in the repository at time t is defined as:<br>
          F_b(t)={1 if p is equal to local copy at time t, 0 otherwise}
          Age: This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time
          t is defined as:<br>
          A_p(t) ={0 if p is not modified at time t,t - modification time of p otherwise}
          </font>
        </p>
        <p align="center"><font face="Arial" size="2" style="margin-bottom:10px">For python code please visit
        <a> https://github.com/vikasdumca/SearchEngine</a>
        </font></td>
    
    </center>
     
    <td width="1%" valign="top">
      <p align="right"><img border="0" src="img/but-top.jpg" width="136" height="22"><br>
      <img border="0" src="img/button1.jpg" width="136" height="43"><br>
      <img border="0" src="img/button2.jpg" width="136" height="50"><br>
      <img border="0" src="img/button3.jpg" width="136" height="49"><br>
      <img border="0" src="img/button4.jpg" width="136" height="52"><br>
      <img border="0" src="img/butbot.jpg"  width="136" height="40"></td>
  </tr>
  </table>
</div>
<div align="center">
  <center>
  <!--table border="0" width="90%" cellspacing="0" cellpadding="0" background="img/bluebar.jpg"-->
    <tr>
      <td width="100%"><font size="1">&nbsp;</font></td>
    </tr>
  </table>
  </center>
</div>

</body>

</html>

